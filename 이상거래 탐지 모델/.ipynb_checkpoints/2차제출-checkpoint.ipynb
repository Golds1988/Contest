{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# file submit\n",
    "from nipa.taskSubmit import nipa_submit\n",
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import os  \n",
    "import tempfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_dir = \"/home/workspace/data/.train/.task150\"\n",
    "file_list = os.listdir(path_dir)\n",
    "# print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df = pd.read_feather(\"/home/workspace/data/.train/.task150/train.feather\", columns=None, use_threads=True)\n",
    "\n",
    "cp_df.sort_values(by=['REQ_DD'], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=pd.merge(raw_df[raw_df['target']==0].groupby(['PAYR_SEQ'])[['target']].count(), raw_df[raw_df['target']==1].groupby(['PAYR_SEQ'])[['target']].count(), how='right',on='PAYR_SEQ')\n",
    "# d1.rename(columns = {'old_nm' : 'new_nm'), inplace = True)\n",
    "d1.fillna(0, inplace=True)\n",
    "d1.columns = ['normal', 'fraud']\n",
    "d1['rate'] = round(d1['fraud'] /  (d1['normal'] + d1['fraud']) *100,2)\n",
    "d1.sort_values(by='rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=pd.merge(cp_df[cp_df['target']==0].groupby(['PAYR_SEQ'])[['target']].count(), cp_df[cp_df['target']==1].groupby(['PAYR_SEQ'])[['target']].count(), how='right',on='PAYR_SEQ')\n",
    "# d1.rename(columns = {'old_nm' : 'new_nm'), inplace = True)\n",
    "d1.fillna(0, inplace=True)\n",
    "d1.columns = ['normal', 'fraud']\n",
    "d1['rate'] = round(d1['fraud'] /  (d1['normal'] + d1['fraud']) *100,2)\n",
    "d1.sort_values(by='rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df[cp_df['PAYR_SEQ']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd =[cp_df['target']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df['MM_LMT_AMT']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0          362601.419643\n",
    "1          362601.419643\n",
    "2          500000.000000\n",
    "3          362601.419643\n",
    "4          362601.419643\n",
    "               ...      \n",
    "7866543    600000.000000\n",
    "7866544    600000.000000\n",
    "7866545    600000.000000\n",
    "7866546    600000.000000\n",
    "7866547    362601.419643\n",
    "Name: MM_LMT_AMT, Length: 7866548, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df['MM_LMT_AMT'].fillna(99999999,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['MM_LMT_AMT'] == 362601.41962302267"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0           True\n",
    "1           True\n",
    "2          False\n",
    "3           True\n",
    "4           True\n",
    "           ...  \n",
    "7866543    False\n",
    "7866544    False\n",
    "7866545    False\n",
    "7866546    False\n",
    "7866547     True\n",
    "Name: MM_LMT_AMT, Length: 7866548, dtype: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd[cp_df['MM_LMT_AMT'] == 362601.41962302267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"PAY_YM\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0          201907\n",
    "1          201907\n",
    "2          201907\n",
    "3          201907\n",
    "4          201907\n",
    "            ...  \n",
    "7866543    201910\n",
    "7866544    201910\n",
    "7866545    201910\n",
    "7866546    201910\n",
    "7866547    201910\n",
    "Name: PAY_YM, Length: 7866548, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0    7591835\n",
    "1     274713\n",
    "Name: target, dtype: int64\n",
    "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red=cp_df[(cp_df[\"MM_LMT_AMT\"] == 362601.41962302267) & (cp_df[\"target\"] == 0)]#.sort_values(by=['REQ_DD'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red[\"PAYR_SEQ\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Series([], Name: PAYR_SEQ, dtype: int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red[\"PAYR_SEQ\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Series([], Name: PAYR_SEQ, dtype: int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red[\"PAYR_SEQ\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9usv5x5bXf4qii/+D18+Sw==    2650\n",
    "hwM551LVHGbTCvQw4SZ9cQ==    1718\n",
    "7odrSysTECOkLaxpuIjUdQ==    1639\n",
    "3f6SRCcWgco3VuuyQQGOrg==    1426\n",
    "sguxDdoUrnVXozmlkok8cw==    1325\n",
    "                            ... \n",
    "1UgQGvY2rN0ujIoIF8TqNg==       1\n",
    "OzgiNiI46sOrfbli2yjURQ==       1\n",
    "n+XlDFaTcKDOWFjiCJMJYg==       1\n",
    "vtLu/WEj6wpiyju4mq4xag==       1\n",
    "0V2BS1AFXa+TqNyl9bJy1g==       1\n",
    "Name: PAYR_SEQ, Length: 1857894, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=raw_df[raw_df['PAYR_SEQ']=='9usv5x5bXf4qii/+D18+Sw==']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_df['MM_LMT_AMT'] == 99999999 & raw_df['target'] == 1:\n",
    "    print (raw_df)\n",
    "else:\n",
    "    print(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"PAYR_SEQ\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps1=raw_df[[\"PAYR_SEQ\",\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg=raw_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0    7591835\n",
    "1     274713\n",
    "Name: target, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps1 = pd.DataFrame(raw_df[\"PAYR_SEQ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps1.insert(1, 'tar', raw_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_df = raw_df.copy()\n",
    "ndf  = cp_df[cp_df['target']==0]\n",
    "fdf  = cp_df[cp_df['target']==1]\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf  = raw_df[cp_df['target']==0]\n",
    "fdf  = raw_df[cp_df['target']==1]\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df[cp_df['target']==1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxx = fdf[['PAYR_SEQ']].value_counts()\n",
    "\n",
    "nxx = ndf[['PAYR_SEQ']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps1[[\"target\"] == 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fxx)\n",
    "print()\n",
    "print(nxx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PAYR_SEQ                \n",
    "V5c/wlrqJh8qNd1C1yv5aQ==    205\n",
    "9vsF57Jf9ivOzbMZneMLWg==    146\n",
    "+QwM9wOfFxSUZ7JjPJvrzw==    117\n",
    "/6/ryx1a52Z9ciKWBLGreg==    103\n",
    "WKT2vBYj5wMuBAxODiDkpA==    101\n",
    "dtype: int64\n",
    "\n",
    "PAYR_SEQ                \n",
    "G2yktsFZB1KDyFZwaXqlZQ==    3132\n",
    "9usv5x5bXf4qii/+D18+Sw==    2650\n",
    "82SAG197ep0TNzK8FaRqPA==    1915\n",
    "t3Vx8NQWx/ST3189t/fcQQ==    1844\n",
    "hwM551LVHGbTCvQw4SZ9cQ==    1718\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelEncoder()\n",
    "\n",
    "# raw_df[\"PAYR_SEQ_LE\"] = enc.fit_transform(raw_df[\"PAYR_SEQ\"])\n",
    "# raw_df[\"MPHN_NO_LE\"] = enc.fit_transform(raw_df[\"MPHN_NO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "fk1 = pd.DataFrame(raw_df1)\n",
    "list(fk1)\n",
    "fk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "         PAYR_SEQ_LE  MPHN_NO_LE\n",
    "0                3.0           2\n",
    "1                4.0           1\n",
    "2                1.0           1\n",
    "3                1.0           2\n",
    "4                5.0           1\n",
    "...              ...         ...\n",
    "2854486          NaN           1\n",
    "2854487          NaN           2\n",
    "2854488          NaN           1\n",
    "2854489          NaN           1\n",
    "2854490          NaN           1\n",
    "\n",
    "[2854491 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk2 = pd.DataFrame(raw_df2)\n",
    "print(fk['MPHN_NO_LE'].value_counts())\n",
    "print()\n",
    "print(fk['PAYR_SEQ_LE'].value_counts())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1       1558572\n",
    "2        516887\n",
    "3        248512\n",
    "4        144017\n",
    "5         92062\n",
    "         ...   \n",
    "295           1\n",
    "289           1\n",
    "287           1\n",
    "284           1\n",
    "1916          1\n",
    "Name: MPHN_NO_LE, Length: 370, dtype: int64\n",
    "\n",
    "1.0      1520775\n",
    "2.0       507499\n",
    "3.0       244239\n",
    "4.0       141608\n",
    "5.0        90311\n",
    "          ...   \n",
    "304.0          1\n",
    "302.0          1\n",
    "301.0          1\n",
    "298.0          1\n",
    "511.0          1\n",
    "Name: PAYR_SEQ_LE, Length: 370, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fkm = pd.merge(fk1,fk2, how='outer',on='')\n",
    "fk = pd.concat([fk1,fk2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df\n",
    "df.insert(3,\"p\",[],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df1 = raw_df[\"PAYR_SEQ\"].value_counts()\n",
    "raw_df2 = raw_df[\"MPHN_NO\"].value_counts()\n",
    "diff = raw_df[[raw_df1!= raw_df2]][raw_df1,raw_df2]\n",
    "\n",
    "\n",
    "# diff.value_counts()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = raw_df[[raw_df1!= raw_df2]][raw_df1,raw_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"PAYR_SEQ\"].nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2798772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"MPHN_NO\"].nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2854491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df2.zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df1.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = pd.DataFrame(raw_df[\"PAYR_SEQ\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1.insert(1, 'Name', raw_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2.insert(1, 'Name', raw_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1.insert(2,'Name2', raw_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = raw1['Name'] != raw2['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df1 = raw_df1.partition('==')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = raw1[[1 != 2]][1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = pd.DataFrame(raw_df[\"PAYR_SEQ_LE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = pd.DataFrame(raw_df[\"MPHN_NO\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = np.array(raw1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = np.array(raw2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([ 538267,  865870, 1938115, ..., 1347223,  563023, 2638308])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw1 = pd.DataFrame(raw1, columns = ['PAYR_SEQ_LE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw1['index'] = index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no1 = []\n",
    "no2 = []\n",
    "a = 0\n",
    "b = 0\n",
    "for i in range(len(index1)):\n",
    "    a+=1\n",
    "    no1.append(a)\n",
    "for j in range(len(index2)):\n",
    "    b+=1\n",
    "    no2.append(b)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1['no'] = no1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2['no'] = no2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df2 = pd.merge(raw1,raw2 ,how='outer',on='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_df2['PAYR_SEQ_LE'].isnull().value_counts())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "False    2798772\n",
    "True       55719\n",
    "Name: PAYR_SEQ_LE, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cp_df.columns\n",
    "flist=[]\n",
    "ulist=[]\n",
    "plist=[]\n",
    "for feature in features:\n",
    "   unique_cnt = cp_df[feature].nunique()\n",
    "   flist.append(feature)\n",
    "   ulist.append(unique_cnt)\n",
    "   plist.append(unique_cnt/cp_df.shape[0]*100)\n",
    "mydic = {\"uniq_cnt\":ulist, \"Percent\":plist}\n",
    "rdf = pd.DataFrame(mydic, index = flist).sort_values(by=['Percent'], axis=0, ascending=False).T\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(raw_df['target'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Examples:\n",
    "    Total: 7866548\n",
    "    Positive: 274713 (3.49% of total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 차트 그리기 공통 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------평가 차트 그리기 공통 함수\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "              plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "              plt.ylim([0.8,1])\n",
    "        else:\n",
    "              plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "\n",
    "##-----------------------------------------------------오차행렬 공통 함수\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "    print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "\n",
    "##-----------------------------------------------------## ROC 플로팅 공통 함수\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,20])\n",
    "    plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답안 제출용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_SUBMIT_FILE_ = '/home/workspace/data/baseline/task150/prediction/prediction.feather'\n",
    "DEST_SUBMIT_FILE_='/home/workspace/user-workspace/prediction150/'\n",
    "team_id=\"1288\"\n",
    "task_no= \"150\"\n",
    "    \n",
    "def submit_func(test_pred_proba, name_option=None) :\n",
    "    sub_df = pd.read_feather(SAMPLE_SUBMIT_FILE_) \n",
    "    sub_df['target'] = test_pred_proba \n",
    "    \n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['TRD_NO'] = key_test\n",
    "    submit_df[\"target\"] = test_pred_proba\n",
    "#     print(submit_df.head())\n",
    "    DEST_SUBMIT_FILE_ = DEST_SUBMIT_FILE_ + \"prediction\"+name_option+\".feather\"\n",
    "    sub_df.to_feather(DEST_SUBMIT_FILE_)\n",
    "\n",
    "    # 파일 존재 여부 확인\n",
    "    if(os.path.isfile(DEST_SUBMIT_FILE_)) :\n",
    "        nipa_submit(team_id=team_id, task_no=task_no,result=DEST_SUBMIT_FILE_)\n",
    "        print(\"제출 성공\")\n",
    "    else:\n",
    "        print(\"제출 실패 : file not created\")\n",
    "    check = pd.read_feather(DEST_SUBMIT_FILE_) \n",
    "    return check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "          keras.layers.Dense(16, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "          keras.layers.Dropout(0.5),\n",
    "          keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias),\n",
    "      ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cp_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 7866548 entries, 0 to 7866547\n",
    "Data columns (total 41 columns):\n",
    " #   Column            Dtype  \n",
    "---  ------            -----  \n",
    " 0   TRD_NO            object \n",
    " 1   REQ_DD            int64  \n",
    " 2   CP_CD             object \n",
    " 3   CP_NM             object \n",
    " 4   GODS_NM           object \n",
    " 5   PAYR_SEQ          object \n",
    " 6   MPHN_NO           object \n",
    " 7   COMMC_CLF         object \n",
    " 8   AC_PAY_AMT        int64  \n",
    " 9   NPAY_YN           object \n",
    " 10  PAY_MTHD_CD       object \n",
    " 11  MM_LMT_AMT        float64\n",
    " 12  REMD_LMT_AMT      float64\n",
    " 13  ARS_AUTHTI_YN     object \n",
    " 14  PAYR_IP           object \n",
    " 15  SUB_IP_A          object \n",
    " 16  SUB_IP_B          object \n",
    " 17  SUB_IP_C          object \n",
    " 18  SUB_IP_D          object \n",
    " 19  FGPT              object \n",
    " 20  AGE               int64  \n",
    " 21  GNDR              object \n",
    " 22  FOREI_YN          object \n",
    " 23  SMS_RE_SND_CNT    int64  \n",
    " 24  AUTHTI_CLF_FLG    object \n",
    " 25  ACUM_RCPT_AMT     int64  \n",
    " 26  PAY_YM            int64  \n",
    " 27  SVC_CLF_NM        object \n",
    " 28  CP_M_CLF_NM       object \n",
    " 29  CP_S_CLF_NM       object \n",
    " 30  NPAY_AMT_24M      float64\n",
    " 31  MAX_NPAY_CNT_24M  int64  \n",
    " 32  TRD_CNT_6M        int64  \n",
    " 33  REAL_TRD_CNT_6M   int64  \n",
    " 34  NIGHT_TRD_RT_6M   float64\n",
    " 35  AVG_AMT_6M        float64\n",
    " 36  MAX_LMT_3M_RT     float64\n",
    " 37  NPAY_CNT_24M      int64  \n",
    " 38  NPAY_CNT_12MNTS   int64  \n",
    " 39  NPAY_AMT_60M      float64\n",
    " 40  target            int64  \n",
    "dtypes: float64(7), int64(12), object(22)\n",
    "memory usage: 2.4+ GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_value_column = ['SMS_RE_SND_CNT','NPAY_AMT_24M','MAX_NPAY_CNT_24M' ,'REAL_TRD_CNT_6M','NIGHT_TRD_RT_6M','NPAY_CNT_24M','NPAY_CNT_12MNTS','NPAY_AMT_60M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TRD_NO              0\n",
    "REQ_DD              0\n",
    "CP_CD               0\n",
    "CP_NM               0\n",
    "PAYR_SEQ            0\n",
    "MPHN_NO             0\n",
    "COMMC_CLF           0\n",
    "AC_PAY_AMT          0\n",
    "NPAY_YN             0\n",
    "PAY_MTHD_CD         0\n",
    "MM_LMT_AMT          0\n",
    "PAYR_IP             0\n",
    "SUB_IP_A            0\n",
    "SUB_IP_B            0\n",
    "SUB_IP_C            0\n",
    "SUB_IP_D            0\n",
    "FGPT                0\n",
    "AGE                 0\n",
    "GNDR                0\n",
    "FOREI_YN            0\n",
    "SMS_RE_SND_CNT      0\n",
    "AUTHTI_CLF_FLG      0\n",
    "ACUM_RCPT_AMT       0\n",
    "PAY_YM              0\n",
    "SVC_CLF_NM          0\n",
    "CP_M_CLF_NM         0\n",
    "CP_S_CLF_NM         0\n",
    "NPAY_AMT_24M        0\n",
    "MAX_NPAY_CNT_24M    0\n",
    "TRD_CNT_6M          0\n",
    "REAL_TRD_CNT_6M     0\n",
    "NIGHT_TRD_RT_6M     0\n",
    "AVG_AMT_6M          0\n",
    "MAX_LMT_3M_RT       0\n",
    "NPAY_CNT_24M        0\n",
    "NPAY_CNT_12MNTS     0\n",
    "NPAY_AMT_60M        0\n",
    "target              0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df['NPAY_YN'].fillna('N',inplace=True)\n",
    "cp_df['PAY_MTHD_CD'].fillna('D',inplace=True)\n",
    "cp_df['CP_S_CLF_NM'].fillna('게임',inplace=True)\n",
    "cp_df['MM_LMT_AMT'].fillna(cp_df['MM_LMT_AMT'].mean(),inplace=True)\n",
    "cp_df['CP_M_CLF_NM'].fillna('게임',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df.drop(['GODS_NM','REMD_LMT_AMT','ARS_AUTHTI_YN'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불피요한 피쳐 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = ['TRD_NO','CP_CD', 'CP_NM','NPAY_CNT_24M','NPAY_AMT_60M', 'GODS_NM','NPAY_CNT_12MNTS','NIGHT_TRD_RT_6M','AVG_AMT_6M','MAX_LMT_3M_RT', 'COMMC_CLF','PAY_MTHD_CD','ARS_AUTHTI_YN','CP_M_CLF_NM',\n",
    "                       'MPHN_NO', 'PAYR_IP', 'SUB_IP_A','MAX_NPAY_CNT_24M','TRD_CNT_6M','REAL_TRD_CNT_6M','SUB_IP_B','FOREI_YN','AGE','CP_S_CLF_NM','NPAY_AMT_24M','SMS_RE_SND_CNT',\n",
    "                       'SUB_IP_C', 'SUB_IP_D', 'FGPT','AUTHTI_CLF_FLG','ACUM_RCPT_AMT','PAY_YM','SVC_CLF_NM']\n",
    "raw_df.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = ['TRD_NO','CP_CD', 'CP_NM','NPAY_CNT_24M','NPAY_AMT_60M', 'GODS_NM','NPAY_CNT_12MNTS','NIGHT_TRD_RT_6M','AVG_AMT_6M','MAX_LMT_3M_RT', 'COMMC_CLF','PAY_MTHD_CD','ARS_AUTHTI_YN','CP_M_CLF_NM',\n",
    "                       'MPHN_NO', 'PAYR_IP', 'SUB_IP_A','MAX_NPAY_CNT_24M','TRD_CNT_6M',\n",
    "                       'SUB_IP_C', 'SUB_IP_D', 'FGPT','AUTHTI_CLF_FLG','ACUM_RCPT_AMT','PAY_YM','SVC_CLF_NM']\n",
    "raw_df.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.drop('NPAY_YN', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cp_df.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df['MM_LMT_AMT'].fillna(9999999,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cp_df.isnull().sum()\n",
    "cp_df.isnull().sum()\n",
    "TRD_NO                    0\n",
    "REQ_DD                    0\n",
    "CP_CD                     0\n",
    "CP_NM                     0\n",
    "GODS_NM                  21\n",
    "PAYR_SEQ                  0\n",
    "MPHN_NO                   0\n",
    "COMMC_CLF                 0\n",
    "AC_PAY_AMT                0\n",
    "NPAY_YN               10335\n",
    "PAY_MTHD_CD         5229313\n",
    "MM_LMT_AMT          5231802\n",
    "REMD_LMT_AMT          10335\n",
    "ARS_AUTHTI_YN         10728\n",
    "PAYR_IP                   0\n",
    "SUB_IP_A                  0\n",
    "SUB_IP_B                  0\n",
    "SUB_IP_C                  0\n",
    "SUB_IP_D                  0\n",
    "FGPT                      0\n",
    "AGE                       0\n",
    "GNDR                      0\n",
    "FOREI_YN                  0\n",
    "SMS_RE_SND_CNT            0\n",
    "AUTHTI_CLF_FLG            0\n",
    "ACUM_RCPT_AMT             0\n",
    "PAY_YM                    0\n",
    "SVC_CLF_NM                0\n",
    "CP_M_CLF_NM           69294\n",
    "CP_S_CLF_NM           69294\n",
    "NPAY_AMT_24M              0\n",
    "MAX_NPAY_CNT_24M          0\n",
    "TRD_CNT_6M                0\n",
    "REAL_TRD_CNT_6M           0\n",
    "NIGHT_TRD_RT_6M           0\n",
    "AVG_AMT_6M                0\n",
    "MAX_LMT_3M_RT             0\n",
    "NPAY_CNT_24M              0\n",
    "NPAY_CNT_12MNTS           0\n",
    "NPAY_AMT_60M              0\n",
    "target                    0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df['MM_LMT_AMT'].mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "362601.41962302267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_columns = cp_df.select_dtypes(include='object').columns\n",
    "print(enc_columns)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in enc_columns:\n",
    "    cp_df[col] = enc.fit_transform(cp_df[col])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Index(['TRD_NO', 'CP_CD', 'CP_NM', 'PAYR_SEQ', 'MPHN_NO', 'COMMC_CLF',\n",
    "       'NPAY_YN', 'PAY_MTHD_CD', 'PAYR_IP', 'SUB_IP_A', 'SUB_IP_B', 'SUB_IP_C',\n",
    "       'SUB_IP_D', 'FGPT', 'GNDR', 'FOREI_YN', 'AUTHTI_CLF_FLG', 'SVC_CLF_NM',\n",
    "       'CP_M_CLF_NM', 'CP_S_CLF_NM'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df.drop(enc_columns, axis=1, inplace=True) \n",
    "# cp_df.drop(['PAY_YM','REQ_DD'], axis=1, inplace=True)  -------------------------- REQ_DD 이후 복원............................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMT 스케일링 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `Amount` column covers a huge range. Convert to log-space.\n",
    "# 결측처리로 대다수가 삭제된 상태\n",
    "# amt_col = ['AC_PAY_AMT','MM_LMT_AMT','REMD_LMT_AMT','AVG_AMT_6M','ACUM_RCPT_AMT','NPAY_AMT_24M']\n",
    "amt_col = ['MM_LMT_AMT','AVG_AMT_6M']\n",
    "\n",
    "\n",
    "eps=0.001 # 0 => 0.1¢\n",
    "cols = amt_col\n",
    "for c in cols:\n",
    "    cp_df[c] = np.log(cp_df[c]+eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cp_df을 3개로 분리  : train_df, test_df , val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(cp_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df['target'])\n",
    "bool_train_labels = train_labels != 0\n",
    "train_df = train_df.drop('target' , axis=1)\n",
    "train_features = np.array(train_df)\n",
    "\n",
    "val_labels = np.array(val_df['target'])\n",
    "val_df = val_df.drop('target' , axis=1)\n",
    "val_features = np.array(val_df)\n",
    "\n",
    "test_labels = np.array(test_df['target'])\n",
    "test_df = test_df.drop('target' , axis=1)\n",
    "test_features = np.array(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training labels shape: (5034590,)\n",
    "Validation labels shape: (1258648,)\n",
    "Test labels shape: (1573310,)\n",
    "Training features shape: (5034590, 29)\n",
    "Validation features shape: (1258648, 29)\n",
    "Test features shape: (1573310, 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target 별로 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
    "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가척도 및 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.summary()\n",
    " \n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "model = make_model(output_bias = initial_bias)\n",
    "model.predict(train_features[:10])\n",
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))\n",
    "\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "print(\"tempfile.mkdtemp\", tempfile.mkdtemp())\n",
    "model.save_weights(initial_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_4 (Dense)              (None, 16)                480       \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 16)                0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 1)                 17        \n",
    "=================================================================\n",
    "Total params: 497\n",
    "Trainable params: 497\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Loss: 0.1675\n",
    "tempfile.mkdtemp /tmp/tmpmwyjbm4t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 편향 재계산후 학습 ==================== 3차 학습¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##-----------------------------------------------------## 클래스 가중치 계산\n",
    "# # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# # The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "# weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "# print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "\n",
    "# weighted_model = make_model()\n",
    "# weighted_model.load_weights(initial_weights)\n",
    "\n",
    "# weighted_history = weighted_model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks = [early_stopping],\n",
    "#     validation_data=(val_features, val_labels),\n",
    "#     # The class weights go here\n",
    "#     class_weight=class_weight) #----------------------------학습 시 가중치 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(weighted_history)  #------------------------ CURV 확인\n",
    "\n",
    "# train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)   #------------------------ 오차행렬\n",
    "# test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "# weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "#                                            batch_size=BATCH_SIZE, verbose=0)\n",
    "# for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "#     print(name, ': ', value)\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 1차 제출\n",
    "* TRD_NO 에 대한 모델 예측 proba\n",
    "\n",
    "* Submission(예시, prediction.feather)\n",
    "TRD_NO\ttarget<br>\n",
    "nv97RPvpsILRkO0BLxDZgs8BHzilzEv8TzSWunbrJsA=,\t0.5\n",
    "81Qha5XTB9l/meAOl1KBPIqYcroUxJ1L4dJM7JpKZSA=\t0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"./model150_1_.hdf5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"./model150_1_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model(\"./model150_1_.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출용 test 파일 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_feather('/home/workspace/data/.train/.task150/test.feather')\n",
    "\n",
    "\n",
    "key_test = x_test.iloc[:, 0]  #'TRD_NO'\n",
    "x_test = x_test.iloc[:, 1:]\n",
    "\n",
    "\n",
    "x_test['NPAY_YN'].fillna('N',inplace=True)\n",
    "x_test['PAY_MTHD_CD'].fillna('D',inplace=True)\n",
    "x_test['CP_S_CLF_NM'].fillna('게임',inplace=True)\n",
    "x_test['MM_LMT_AMT'].fillna(x_test['MM_LMT_AMT'].mean(),inplace=True)\n",
    "x_test['CP_M_CLF_NM'].fillna('게임',inplace=True)\n",
    "\n",
    "drop_column = ['CP_NM','GODS_NM','REMD_LMT_AMT','ARS_AUTHTI_YN']\n",
    "x_test.drop(drop_column, axis=1, inplace=True)\n",
    "\n",
    "enc_columns = x_test.select_dtypes(include='object').columns\n",
    "enc = LabelEncoder()\n",
    "for col in enc_columns:\n",
    "    x_test[col] = enc.fit_transform(x_test[col])\n",
    "\n",
    "# x_test.drop(enc_columns, axis=1, inplace=True)\n",
    "# x_test.drop(['PAY_YM','REQ_DD'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "amt_col = ['MM_LMT_AMT','AVG_AMT_6M']\n",
    "eps=0.001 # 0 => 0.1¢\n",
    "cols = amt_col\n",
    "for c in cols:\n",
    "    x_test[c] = np.log(x_test[c]+eps)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "scaler = StandardScaler()\n",
    "x_test_features = scaler.fit_transform(x_test)\n",
    "x_test_features = np.clip(x_test_features, -5, 5)\n",
    "print('Test features shape:', x_test_features.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test features shape: (3910929, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# x_test_pred = model.predict(x_test_features, batch_size=2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_pred_proba = model.predict_proba(x_test_features, batch_size=2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = submit_func(x_test_pred_proba, \"1\")\n",
    "# check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_split(df, val_ym='201910'):\n",
    "#     val_idx = df['REQ_DD'].apply(lambda x: str(x)[:6]) == val_ym\n",
    "#     val_df = df[val_idx]\n",
    "#     train_df = df[~val_idx]\n",
    "#     print(f\"Train: {train_df.shape}, Validation: {val_df.shape}\")\n",
    "#     return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오버샘플링 적용후 학습 ==================== 4차 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------- feature / labels 쌍 리턴  공통함수\n",
    "def make_ds(features, labels):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
    "    ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = train_features[bool_train_labels]\n",
    "neg_features = train_features[~bool_train_labels]\n",
    "\n",
    "pos_labels = train_labels[bool_train_labels]\n",
    "neg_labels = train_labels[~bool_train_labels]\n",
    "\n",
    "ids = np.arange(len(pos_features))\n",
    "choices = np.random.choice(ids, len(neg_features))\n",
    "\n",
    "res_pos_features = pos_features[choices]\n",
    "res_pos_labels = pos_labels[choices]\n",
    "\n",
    "print(res_pos_features.shape)\n",
    "\n",
    "resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "# np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "print(resampled_features.shape)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "\n",
    "pos_ds = make_ds(pos_features, pos_labels)\n",
    "neg_ds = make_ds(neg_features, neg_labels)\n",
    "# for features, label in pos_ds.take(1):\n",
    "#   print(\"Features:\\n\", features.numpy())\n",
    "#   print()\n",
    "#   print(\"Label: \", label.numpy())\n",
    "\n",
    "resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
    "resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "# for features, label in resampled_ds.take(1):\n",
    "#   print(label.numpy().mean())\n",
    "\n",
    "\n",
    "##-----------------------------------------------------## neg 1회 출현 최소 epoch\n",
    "resampled_steps_per_epoch = np.ceil(2.0*neg/BATCH_SIZE)\n",
    "print(resampled_steps_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "##-----------------------------------------------------## 오버 샘플링 학습\n",
    "# model = load_model(\"./model150_1_.hdf5\")\n",
    "resampled_model = make_model()\n",
    "resampled_model.load_weights(initial_weights)\n",
    "\n",
    "# Reset the bias to zero, since this dataset is balanced.\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0])\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(4858945, 29)\n",
    "(9717890, 29)\n",
    "7414.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================long long long \n",
    "resampled_history = resampled_model.fit(\n",
    "    resampled_ds,\n",
    "#     epochs=EPOCHS,\n",
    "    steps_per_epoch= resampled_steps_per_epoch,\n",
    "    epochs=4, #10*EPOCHS, \n",
    "    #callbacks = [early_stopping],\n",
    "    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 1/7\n",
    "7414/7414 [==============================] - 244s 33ms/step - loss: 0.2787 - tp: 6224878.0000 - fp: 569447.0000 - tn: 16738743.0000 - fn: 1719984.0000 - accuracy: 0.9093 - precision: 0.9162 - recall: 0.7835 - auc: 0.9481 - val_loss: 0.1931 - val_tp: 36980.0000 - val_fp: 56652.0000 - val_tn: 1157936.0000 - val_fn: 7080.0000 - val_accuracy: 0.9494 - val_precision: 0.3950 - val_recall: 0.8393 - val_auc: 0.9657\n",
    "Epoch 2/7\n",
    "7414/7414 [==============================] - 246s 33ms/step - loss: 0.2467 - tp: 6263087.0000 - fp: 374824.0000 - tn: 7215363.0000 - fn: 1330598.0000 - accuracy: 0.8877 - precision: 0.9435 - recall: 0.8248 - auc: 0.9591 - val_loss: 0.1841 - val_tp: 36938.0000 - val_fp: 52104.0000 - val_tn: 1162484.0000 - val_fn: 7122.0000 - val_accuracy: 0.9529 - val_precision: 0.4148 - val_recall: 0.8384 - val_auc: 0.9667\n",
    "Epoch 3/7\n",
    "7414/7414 [==============================] - 242s 33ms/step - loss: 0.2446 - tp: 6266126.0000 - fp: 367525.0000 - tn: 7227215.0000 - fn: 1323006.0000 - accuracy: 0.8887 - precision: 0.9446 - recall: 0.8257 - auc: 0.9598 - val_loss: 0.1877 - val_tp: 37103.0000 - val_fp: 54483.0000 - val_tn: 1160105.0000 - val_fn: 6957.0000 - val_accuracy: 0.9512 - val_precision: 0.4051 - val_recall: 0.8421 - val_auc: 0.9670\n",
    "Epoch 4/7\n",
    "5775/7414 [======================>.......] - ETA: 51s - loss: 0.2440 - tp: 4883304.0000 - fp: 283151.0000 - tn: 5631234.0000 - fn: 1029511.0000 - accuracy: 0.8890 - precision: 0.9452 - recall: 0.8259 - auc: 0.9600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_model.save(\"./model150_3_.hdf5\")\n",
    "resampled_model.save_weights(\"./model150_3_weight.hdf5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy\n",
    "# import pandas as pd\n",
    "\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# resampled_history = keras.models.load_model('./model150_3_.hdf5')\n",
    "# resampled_history.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_metrics(resampled_history )\n",
    "\n",
    "train_predictions_resampled = resampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_resampled = resampled_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "resampled_results = resampled_model.evaluate(test_features, test_labels,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cm(test_labels, test_predictions_resampled)\n",
    "\n",
    "# plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "# plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "# plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\n",
    "# plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "# plot_roc(\"Train Resampled\", train_labels, train_predictions_resampled,  color=colors[2])\n",
    "# plot_roc(\"Test Resampled\", test_labels, test_predictions_resampled,  color=colors[2], linestyle='--')\n",
    "# plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_pred = model.predict(x_test_features, batch_size=2048) \n",
    "x_test_pred_proba = model.predict_proba(x_test_features, batch_size=2048) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측한 답안지 최종 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = submit_func(x_test_pred_proba, \"2\")\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SUBMIT_FILE_ = '/home/workspace/data/baseline/task150/prediction/prediction.feather'\n",
    "DEST_SUBMIT_FILE_='/home/workspace/user-workspace/prediction150/'\n",
    "team_id=\"1288\"\n",
    "task_no= \"150\"\n",
    "    \n",
    "def submit_func(test_pred_proba, name_option=None) :\n",
    "    sub_df = pd.read_feather(SAMPLE_SUBMIT_FILE_) \n",
    "    sub_df['target'] = test_pred_proba \n",
    "    \n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['TRD_NO'] = key_test\n",
    "    submit_df[\"target\"] = test_pred_proba\n",
    "#     print(submit_df.head())\n",
    "    DEST_SUBMIT_FILE_ = DEST_SUBMIT_FILE_ + \"prediction\"+name_option+\".feather\"\n",
    "    sub_df.to_feather(DEST_SUBMIT_FILE_)\n",
    "\n",
    "    # 파일 존재 여부 확인\n",
    "    if(os.path.isfile(DEST_SUBMIT_FILE_)) :\n",
    "        nipa_submit(team_id=team_id, task_no=task_no,result=DEST_SUBMIT_FILE_)\n",
    "        print(\"제출 성공\")\n",
    "    else:\n",
    "        print(\"제출 실패 : file not created\")\n",
    "    check = pd.read_feather(DEST_SUBMIT_FILE_) \n",
    "    return check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
