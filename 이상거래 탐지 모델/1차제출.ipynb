{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##태스크 번호 150: 이상거래 탐지 모델 [페이레터]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# file submit\n",
    "from nipa.taskSubmit import nipa_submit\n",
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import os  \n",
    "import tempfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, fbeta_score\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_dir = \"/home/workspace/data/.train/.task150\"\n",
    "file_list = os.listdir(path_dir)\n",
    "# print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_feather(\"/home/workspace/data/.train/.task150/train.feather\", columns=None, use_threads=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df[raw_df['PAYR_SEQ'] =='9usv5x5bXf4qii/+D18+Sw=='].to_csv(\"150what.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target 분포\n",
    "* 비대칭 오버샘플링 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(raw_df['target'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Examples:\n",
    "    Total: 7866548\n",
    "    Positive: 274713 (3.49% of total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_df.shape)\n",
    "# cp_df = raw_df.copy()\n",
    "# ndf  = cp_df[cp_df['target']==0]\n",
    "# fdf  = cp_df[cp_df['target']==1]\n",
    "# # cp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정상거래  이상거래 TOP-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxx = fdf[['PAYR_SEQ']].value_counts().head()\n",
    "# nxx = ndf[['PAYR_SEQ']].value_counts().head()\n",
    "# print(fxx.head(10))\n",
    "# print(nxx.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df['MM_LMT_AMT'].fillna(cp_df['MM_LMT_AMT'].max(),inplace=True) #cp_df['MM_LMT_AMT'].mean()\n",
    "cp_df['REMD_LMT_AMT'].fillna(0,inplace=True)\n",
    "\n",
    "# cp_df['NPAY_YN'].fillna('NNNN',inplace=True)\n",
    "# cp_df['PAY_MTHD_CD'].fillna('DDDD',inplace=True)\n",
    "# cp_df['CP_S_CLF_NM'].fillna('게임게임게임',inplace=True)\n",
    "# cp_df['CP_M_CLF_NM'].fillna('게임게임게임',inplace=True)\n",
    "# cp_df['ARS_AUTHTI_YN'].fillna('NNNN',inplace=True)\n",
    "# cp_df['GODS_NM'].fillna('NNNN',inplace=True)\n",
    "# cp_df['CP_NM'].fillna('NNNN',inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불필요 컬럼 드랍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df.drop(['TRD_NO', 'CP_CD', 'CP_NM', 'GODS_NM', 'MPHN_NO', 'COMMC_CLF', 'NPAY_YN', 'PAY_MTHD_CD',\n",
    "      'ARS_AUTHTI_YN', 'PAYR_IP', 'SUB_IP_A','SUB_IP_B', 'SUB_IP_C', 'SUB_IP_D', 'FGPT', 'AGE', 'GNDR', 'FOREI_YN',\n",
    "       'SMS_RE_SND_CNT', 'AUTHTI_CLF_FLG', 'ACUM_RCPT_AMT',\n",
    "       'SVC_CLF_NM', 'CP_M_CLF_NM', 'CP_S_CLF_NM', 'NPAY_AMT_24M',\n",
    "       'MAX_NPAY_CNT_24M', 'TRD_CNT_6M', 'REAL_TRD_CNT_6M', 'NIGHT_TRD_RT_6M',\n",
    "       'AVG_AMT_6M', 'MAX_LMT_3M_RT', 'NPAY_CNT_24M', 'NPAY_CNT_12MNTS',\n",
    "       'NPAY_AMT_60M'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cp_df[cp_df['PAYR_SEQ']<0]\n",
    "# cp_df['PAYR_SEQ'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encf = cp_df[cp_df['target']==1][['PAYR_SEQ']].value_counts().head()\n",
    "# encn = cp_df[cp_df['target']==0][['PAYR_SEQ']].value_counts().head()\n",
    "# print(encf.head(10))\n",
    "# print(encn.head()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정상 거래 중 사기거래 비율 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1=pd.merge(cp_df[cp_df['target']==0].groupby(['PAYR_SEQ'])[['target']].count(), cp_df[cp_df['target']==1].groupby(['PAYR_SEQ'])[['target']].count(), how='right',on='PAYR_SEQ')\n",
    "# # d1.rename(columns = {'old_nm' : 'new_nm'), inplace = True)\n",
    "# d1.fillna(0, inplace=True)\n",
    "# d1.columns = ['normal', 'fraud']\n",
    "# d1['rate'] = round(d1['fraud'] /  (d1['normal'] + d1['fraud']) *100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(d1.shape , (d1.shape[0] / cp_df.shape[0])*100)\n",
    "# d1.head(20).sort_values(by='fraud', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u1 = ndf[ndf['PAYR_SEQ']==1447968]\n",
    "# u2 = ndf[ndf['PAYR_SEQ']==521322]\n",
    "# u3 = ndf[ndf['PAYR_SEQ']==19673]\n",
    "# u4 = ndf[ndf['PAYR_SEQ']==49469]\n",
    "\n",
    "# u11 = fdf[fdf['PAYR_SEQ']==1447968]\n",
    "# u22 = fdf[fdf['PAYR_SEQ']==521322]\n",
    "# u33 = fdf[fdf['PAYR_SEQ']==19673]\n",
    "# u44 = fdf[fdf['PAYR_SEQ']==49469]\n",
    "\n",
    "# xx = pd.concat([u1, u2,u3, u4,u11, u22,u33, u44])\n",
    "# xx.to_csv('raw_test.csv', sep='\\t')\n",
    "\n",
    "# 1447968    536\n",
    "# 521322     263\n",
    "# 19673      117\n",
    "# 49469      103\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df[cp_df['PAYR_SEQ'] ==790760].to_csv(\"150topf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 차트 그리기 공통 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##-----------------------------------------------------평가 차트 그리기 공통 함수\n",
    "# def plot_metrics(history):\n",
    "#     metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "#     for n, metric in enumerate(metrics):\n",
    "#         name = metric.replace(\"_\",\" \").capitalize()\n",
    "#         plt.subplot(2,2,n+1)\n",
    "#         plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "#         plt.plot(history.epoch, history.history['val_'+metric],\n",
    "#                  color=colors[0], linestyle=\"--\", label='Val')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel(name)\n",
    "#         if metric == 'loss':\n",
    "#               plt.ylim([0, plt.ylim()[1]])\n",
    "#         elif metric == 'auc':\n",
    "#               plt.ylim([0.8,1])\n",
    "#         else:\n",
    "#               plt.ylim([0,1])\n",
    "#         plt.legend()\n",
    "\n",
    "# ##-----------------------------------------------------오차행렬 공통 함수\n",
    "# def plot_cm(labels, predictions, p=0.5):\n",
    "#     cm = confusion_matrix(labels, predictions > p)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "#     plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "#     plt.ylabel('Actual label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "#     print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "#     print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "#     print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "#     print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "#     print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "\n",
    "# ##-----------------------------------------------------## ROC 플로팅 공통 함수\n",
    "# def plot_roc(name, labels, predictions, **kwargs):\n",
    "#     fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "#     plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "#     plt.xlabel('False positives [%]')\n",
    "#     plt.ylabel('True positives [%]')\n",
    "#     plt.xlim([-0.5,20])\n",
    "#     plt.ylim([80,100.5])\n",
    "#     plt.grid(True)\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = cp_df.columns\n",
    "# flist=[]\n",
    "# ulist=[]\n",
    "# plist=[]\n",
    "# for feature in features:\n",
    "#    unique_cnt = cp_df[feature].nunique()\n",
    "#    flist.append(feature)\n",
    "#    ulist.append(unique_cnt)\n",
    "#    plist.append(unique_cnt/cp_df.shape[0]*100)\n",
    "# mydic = {\"uniq_cnt\":ulist, \"Percent\":plist}\n",
    "# rdf = pd.DataFrame(mydic, index = flist).sort_values(by=['Percent'], axis=0, ascending=False).T\n",
    "# rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def missing_data(data):\n",
    "#     total = data.isnull().sum()\n",
    "#     percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "#     tt = pd.concat([total, percent], axis=1, keys=['miss_cnt', 'Percent'])\n",
    "#     types = []\n",
    "#     for col in data.columns:\n",
    "#         dtype = str(data[col].dtype)\n",
    "#         types.append(dtype)\n",
    "#     tt['Types'] = types\n",
    "#     tt = tt.sort_values(by=['Percent'], axis=0, ascending=False)\n",
    "#     return np.transpose(tt)\n",
    "# # %%time\n",
    "# rdf = missing_data(cp_df)\n",
    "# rdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def zero_data(data):\n",
    "#     total = data.isin([0]).sum()\n",
    "#     percent = (data.isin([0]).sum()/data.shape[0]*100)\n",
    "#     tt = pd.concat([total, percent], axis=1, keys=['zero_cnt', 'Percent'])\n",
    "#     types = []\n",
    "#     for col in data.columns:\n",
    "#         dtype = str(data[col].dtype)\n",
    "#         types.append(dtype)\n",
    "#     tt['Types'] = types\n",
    "#     tt = tt.sort_values(by=['Percent'], axis=0, ascending=False)\n",
    "#     return np.transpose(tt)\n",
    "# # %%time\n",
    "# rdf = zero_data(cp_df)\n",
    "# rdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분석을 위한 신규 피쳐 및 가공\n",
    "* PAYR_SEQ : 라벨인코딩\n",
    "* PAY_YM : 년원 라벨인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['PAYR_SEQ_GROUP'] = train_df['PAYR_SEQ']\n",
    "encode_column = ['PAYR_SEQ','PAY_YM']\n",
    "encoder = LabelEncoder()\n",
    "for col in encode_column:\n",
    "    cp_df[col+'_GROUP'] = encoder.fit_transform(cp_df[col])\n",
    "    print(col+'_GROUP', cp_df[col+'_GROUP'].unique())\n",
    "cp_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PAYR_SEQ_GROUP [2326138  453621 1641197 ...  482138 2318097 1837425]\n",
    "PAY_YM_GROUP [0 1 2 3 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_idx = cp_df['REQ_DD'].apply(lambda x: str(x)[:8]) == val_ym\n",
    "cp_df['REQ_DAY'] = cp_df['REQ_DD'].apply(lambda x: str(x)[6:]).astype(np.int32) \n",
    "cp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df.drop(['REQ_DD','PAY_YM','PAYR_SEQ'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df[''].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AC_PAY_AMT  MM_LMT_AMT  REMD_LMT_AMT  target  PAYR_SEQ_GROUP  PAY_YM_GROUP  REQ_DAY\n",
    "770         600000.0    0.0           0       276604          0             3          73\n",
    "                                                                            8          57\n",
    "                                                                            2          41\n",
    "50000       500000.0    500000.0      1       318833          2             16         20\n",
    "                                              2413896         2             1          20\n",
    "                                                                                       ..\n",
    "16870       600000.0    84780.0       0       2271362         3             1           1\n",
    "                        84890.0       0       836217          2             2           1\n",
    "                        96910.0       0       170493          3             2           1\n",
    "                        98840.0       0       1579811         2             21          1\n",
    "1           10000.0     9978.0        0       1901898         1             20          1\n",
    "Length: 7830254, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = cp_df.iloc[:,:3]\n",
    "y = cp_df.iloc[:,3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0    5693775\n",
    "1     206136\n",
    "Name: target, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over,y_train_over = smote.fit_sample(X_train,y_train)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (5899911, 3) (5899911,)\n",
    "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (11387550, 3) (11387550,)\n",
    "SMOTE 적용 후 레이블 값 분포: \n",
    " 1    5693775\n",
    "0    5693775\n",
    "Name: target, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, fbeta_score\n",
    "\n",
    "def metrics(y_test,pred):\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    roc_score = roc_auc_score(y_test,pred)\n",
    "    print('정확도 : {0:.2f}, 정밀도 : {1:.2f}, 재현율 : {2:.2f}'.format(accuracy,precision,recall))\n",
    "    print('f1-score : {0:.2f}, auc : {1:.2f}'.format(f1,roc_score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "\n",
    "# model  = LGBMClassifier(n_estimators=100)\n",
    "# model.fit(X_train_over,y_train_over)\n",
    "# pred = model.predict(X_test)\n",
    "# metrics(y_test,pred)\n",
    "\n",
    "#metrics(y_test,pred)\n",
    "\n",
    "# model  = RandomForestClassifier(n_estimators=100)\n",
    "# model.fit(X_train_over,y_train_over)\n",
    "# pred = model.predict(x_test)\n",
    "# metrics(y_test,pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.booster_.save_model(\"./model150_99_.hdf5\")\n",
    "# model.save(\"./model150_99_.hdf5\")\n",
    "\n",
    "# import sklearn.external.joblib as extjoblib\n",
    "#import joblib\n",
    "# save model\n",
    "#joblib.dump(model, './model150_99_.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#gbm_pickle = joblib.load('./model150_99_.pkl')\n",
    "\n",
    "# re_model = lightgbm.basic.Booster(model_file=\"./model150_99_.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GroupKFold \n",
    "# from xgboost import XGBClassifier \n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train_over,y_train_over)\n",
    "# pred = model.predict(X_test)\n",
    "# metrics(y_test,pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[05:57:27] WARNING: ../src/gbm/gbtree.cc:139: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
    "정확도 : 0.85, 정밀도 : 0.15, 재현율 : 0.72\n",
    "f1-score : 0.24, auc : 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GroupKFold \n",
    "# from xgboost import XGBClassifier \n",
    "    \n",
    "# skf = GroupKFold(n_splits=6)\n",
    "# for i, (idxT, idxV) in enumerate( skf.split(X_train_over, y_train_over\n",
    "#                                             , groups=X_train_over['PAYR_SEQ']) ):\n",
    "#     month = X_train_over.iloc[idxV]['PAYR_SEQ'].iloc[0]\n",
    "#     print('Fold',i,'withholding month',month)\n",
    "#     print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "#     clf = XGBClassifier(\n",
    "#         n_estimators=5000,\n",
    "#         max_depth=12,\n",
    "#         learning_rate=0.02,\n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=0.4,\n",
    "#         missing=-1,\n",
    "#         eval_metric='auc',\n",
    "#         # USE CPU\n",
    "# #             nthread=4,\n",
    "#         #tree_method='hist'\n",
    "#         # USE GPU\n",
    "# #             tree_method='gpu_hist' \n",
    "#     )        \n",
    "#     h = clf.fit(X_train_over.iloc[idxT], y_train_over.iloc[idxT], \n",
    "#             eval_set=[(X_train_over.iloc[idxV],y_train_over.iloc[idxV])],\n",
    "#             verbose=100, early_stopping_rounds=200)\n",
    "\n",
    "#     oof[idxV] += clf.predict_proba(X_train_over.iloc[idxV])[:,1]\n",
    "#     preds += clf.predict_proba(X_train_over)[:,1]/skf.n_splits\n",
    "#     del h, clf\n",
    "#     x=gc.collect()\n",
    "# print('#'*20)\n",
    "# print ('XGB95 OOF CV=',roc_auc_score(y_train_over,oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  clf = xgb.XGBClassifier( \n",
    "#         n_estimators=2000,\n",
    "#         max_depth=12, \n",
    "#         learning_rate=0.02, \n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=0.4, \n",
    "#         missing=-1, \n",
    "#         eval_metric='auc',\n",
    "#         # USE CPU\n",
    "#         #nthread=4,\n",
    "#         #tree_method='hist' \n",
    "#         # USE GPU\n",
    "#         tree_method='gpu_hist' \n",
    "#     )  \n",
    "#     h = clf.fit(X_train_over.loc[idxT,cols], y_train_over[idxT], \n",
    "#         eval_set=[(X_train_over.loc[idxV,cols],y_train_over[idxV])],\n",
    "#         verbose=50, early_stopping_rounds=100)\n",
    "    \n",
    "# ------------------------------ feature importance\n",
    "# cols = cp_df.columns\n",
    "# feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,cols)), columns=['Value','Feature'])\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n",
    "# plt.title('XGB95 Most Important Features')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답안 제출용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_SUBMIT_FILE_ = '/home/workspace/data/baseline/task150/prediction/prediction.feather'\n",
    "DEST_SUBMIT_FILE_='/home/workspace/user-workspace/prediction150/'\n",
    "team_id=\"1288\"\n",
    "task_no= \"150\"\n",
    "    \n",
    "def submit_func(test_pred_proba, name_option=None) :\n",
    "    sub_df = pd.read_feather(SAMPLE_SUBMIT_FILE_) \n",
    "    sub_df['target'] = test_pred_proba \n",
    "    \n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['TRD_NO'] = key_test\n",
    "    submit_df[\"target\"] = test_pred_proba\n",
    "#     print(submit_df.head())\n",
    "    DEST_SUBMIT_FILE_ = DEST_SUBMIT_FILE_ + \"prediction\"+name_option+\".feather\"\n",
    "    sub_df.to_feather(DEST_SUBMIT_FILE_)\n",
    "\n",
    "    # 파일 존재 여부 확인\n",
    "    if(os.path.isfile(DEST_SUBMIT_FILE_)) :\n",
    "        nipa_submit(team_id=team_id, task_no=task_no,result=DEST_SUBMIT_FILE_)\n",
    "        print(\"제출 성공\")\n",
    "    else:\n",
    "        print(\"제출 실패 : file not created\")\n",
    "    check = pd.read_feather(DEST_SUBMIT_FILE_) \n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred_proba = model.predict_proba(X_train_over)\n",
    "check = submit_func(x_test_pred_proba, \"2\")\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cp_df.columns\n",
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,cols)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n",
    "plt.title('XGB95 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS = [\n",
    "#       keras.metrics.TruePositives(name='tp'),\n",
    "#       keras.metrics.FalsePositives(name='fp'),\n",
    "#       keras.metrics.TrueNegatives(name='tn'),\n",
    "#       keras.metrics.FalseNegatives(name='fn'), \n",
    "#       keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#       keras.metrics.Precision(name='precision'),\n",
    "#       keras.metrics.Recall(name='recall'),\n",
    "#       keras.metrics.AUC(name='auc'),\n",
    "# ]\n",
    "\n",
    "# def make_model(metrics = METRICS, output_bias=None):\n",
    "#     if output_bias is not None:\n",
    "#         output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "#     model = keras.Sequential([\n",
    "#           keras.layers.Dense(16, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "#           keras.layers.Dropout(0.5),\n",
    "#           keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias),\n",
    "#       ])\n",
    "\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "#                   loss=keras.losses.BinaryCrossentropy(),\n",
    "#                   metrics=metrics)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# EPOCHS = 100\n",
    "# BATCH_SIZE = 2048\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_auc', \n",
    "#     verbose=1,\n",
    "#     patience=10,\n",
    "#     mode='max',\n",
    "#     restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cp_df을 3개로 분리  : train_df, test_df , val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a utility from sklearn to split and shuffle our dataset.\n",
    "# train_df, test_df = train_test_split(cp_df, test_size=0.2)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# # Form np arrays of labels and features.\n",
    "# train_labels = np.array(train_df['target'])\n",
    "# bool_train_labels = train_labels != 0\n",
    "# train_df = train_df.drop('target' , axis=1)\n",
    "# train_features = np.array(train_df)\n",
    "\n",
    "# val_labels = np.array(val_df['target'])\n",
    "# val_df = val_df.drop('target' , axis=1)\n",
    "# val_features = np.array(val_df)\n",
    "\n",
    "# test_labels = np.array(test_df['target'])\n",
    "# test_df = test_df.drop('target' , axis=1)\n",
    "# test_features = np.array(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# train_features = scaler.fit_transform(train_features)\n",
    "# val_features = scaler.transform(val_features)\n",
    "# test_features = scaler.transform(test_features)\n",
    "\n",
    "# train_features = np.clip(train_features, -5, 5)\n",
    "# val_features = np.clip(val_features, -5, 5)\n",
    "# test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "# print('Training labels shape:', train_labels.shape)\n",
    "# print('Validation labels shape:', val_labels.shape)\n",
    "# print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "# print('Training features shape:', train_features.shape)\n",
    "# print('Validation features shape:', val_features.shape)\n",
    "# print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target 별로 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
    "# neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가척도 및 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# model.summary()\n",
    " \n",
    "\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# model = make_model(output_bias = initial_bias)\n",
    "# model.predict(train_features[:10])\n",
    "# results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))\n",
    "\n",
    "\n",
    "# initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "# print(\"tempfile.mkdtemp\", tempfile.mkdtemp())\n",
    "# model.save_weights(initial_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 편향 재계산후 학습 ==================== 3차 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##-----------------------------------------------------## 클래스 가중치 계산\n",
    "# # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# # The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "# weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "# print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "\n",
    "# weighted_model = make_model()\n",
    "# weighted_model.load_weights(initial_weights)\n",
    "\n",
    "# weighted_history = weighted_model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks = [early_stopping],\n",
    "#     validation_data=(val_features, val_labels),\n",
    "#     # The class weights go here\n",
    "#     class_weight=class_weight) #----------------------------학습 시 가중치 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(weighted_history)  #------------------------ CURV 확인\n",
    "\n",
    "# train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)   #------------------------ 오차행렬\n",
    "# test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "# weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "#                                            batch_size=BATCH_SIZE, verbose=0)\n",
    "# for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "#     print(name, ': ', value)\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1차 제출\n",
    "* TRD_NO 에 대한 모델 예측 proba\n",
    "\n",
    "* Submission(예시, prediction.feather)\n",
    "TRD_NO\ttarget<br>\n",
    "nv97RPvpsILRkO0BLxDZgs8BHzilzEv8TzSWunbrJsA=,\t0.5\n",
    "81Qha5XTB9l/meAOl1KBPIqYcroUxJ1L4dJM7JpKZSA=\t0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"./model150_1_.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"./model150_1_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model(\"./model150_1_.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출용 test 파일 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = pd.read_feather('/home/workspace/data/.train/.task150/test.feather')\n",
    "\n",
    "\n",
    "# key_test = x_test.iloc[:, 0]  #'TRD_NO'\n",
    "# x_test = x_test.iloc[:, 1:]\n",
    "\n",
    "\n",
    "# x_test['NPAY_YN'].fillna('N',inplace=True)\n",
    "# x_test['PAY_MTHD_CD'].fillna('D',inplace=True)\n",
    "# x_test['CP_S_CLF_NM'].fillna('게임',inplace=True)\n",
    "# x_test['MM_LMT_AMT'].fillna(x_test['MM_LMT_AMT'].mean(),inplace=True)\n",
    "# x_test['CP_M_CLF_NM'].fillna('게임',inplace=True)\n",
    "\n",
    "# drop_column = ['CP_NM','GODS_NM','REMD_LMT_AMT','ARS_AUTHTI_YN']\n",
    "# x_test.drop(drop_column, axis=1, inplace=True)\n",
    "\n",
    "# enc_columns = x_test.select_dtypes(include='object').columns\n",
    "# enc = LabelEncoder()\n",
    "# for col in enc_columns:\n",
    "#     x_test[col] = enc.fit_transform(x_test[col])\n",
    "\n",
    "# # x_test.drop(enc_columns, axis=1, inplace=True)\n",
    "# # x_test.drop(['PAY_YM','REQ_DD'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# amt_col = ['MM_LMT_AMT','AVG_AMT_6M']\n",
    "# eps=0.001 # 0 => 0.1¢\n",
    "# cols = amt_col\n",
    "# for c in cols:\n",
    "#     x_test[c] = np.log(x_test[c]+eps)\n",
    "\n",
    "# x_test = np.array(x_test)\n",
    "# scaler = StandardScaler()\n",
    "# x_test_features = scaler.fit_transform(x_test)\n",
    "# x_test_features = np.clip(x_test_features, -5, 5)\n",
    "# print('Test features shape:', x_test_features.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# x_test_pred = model.predict(x_test_features, batch_size=2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred_proba = model.predict_proba(x_test_features, batch_size=2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = submit_func(x_test_pred_proba, \"1\")\n",
    "# check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_split(df, val_ym='201910'):\n",
    "#     val_idx = df['REQ_DD'].apply(lambda x: str(x)[:6]) == val_ym\n",
    "#     val_df = df[val_idx]\n",
    "#     train_df = df[~val_idx]\n",
    "#     print(f\"Train: {train_df.shape}, Validation: {val_df.shape}\")\n",
    "#     return train_df, val_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
